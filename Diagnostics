
library("caret")
library("nnet")
library("gbm")
library("ranger")
library("gdata")
library("xtable")
library("ggplot2")
library("gridExtra")

rm(list=ls(all=TRUE))

## The absolute standardized difference function for covariate j
ASD <- function(j) {
  mean1 <- mean(data[,j]*(data[,D]/DgivenGX.Pred/mean(data[,D]/DgivenGX.Pred)))
  mean0 <- mean(data[,j]*((1-data[,D])/(1-DgivenGX.Pred)/mean((1-data[,D])/(1-DgivenGX.Pred))))
  
  var1 <- sum(data[,D]/DgivenGX.Pred)/( (sum(data[,D]/DgivenGX.Pred))^2 - sum((data[,D]/DgivenGX.Pred)^2) ) * sum( data[,D]/DgivenGX.Pred*(data[,j] - mean1)^2 )
  var0 <- sum((1-data[,D])/(1-DgivenGX.Pred))/( sum((1-data[,D])/(1-DgivenGX.Pred))^2 - sum(((1-data[,D])/(1-DgivenGX.Pred))^2) ) * sum( (1-data[,D])/(1-DgivenGX.Pred)*(data[,j] - mean0)^2 )
  
  return( abs((mean1-mean0)/sqrt((var1+var0)/2)*100) )
}

data <- readRDS("/Users/Ang/Desktop/Research/Causal_decomposition/Data/data_cleaned.rds")

data$gender <- as.numeric(data$gender)-1

data$parental_presence <- as.numeric(data$parental_presence)-1

data$n_sib <- as.numeric(data$n_sib)-1

data$urban <- as.numeric(data$urban)-1

data$edu_exp <- as.numeric(data$edu_exp)-2
data$edu_exp[data$edu_exp==-1] <- 0

data$age <- as.numeric(data$age)-1

data$friend_edu_exp <- as.numeric(data$friend_edu_exp)-2
data$friend_edu_exp[data$friend_edu_exp==-1] <- 0

data$foreign_lang <- as.numeric(data$foreign_lang)-1

data$SMSA1 <- NA
data$SMSA1[data$SMSA==0 & !is.na(data$SMSA)] <- 1
data$SMSA1[data$SMSA!=0 & !is.na(data$SMSA)] <- 0
data$SMSA2 <- NA
data$SMSA2[data$SMSA==1 & !is.na(data$SMSA)] <- 1
data$SMSA2[data$SMSA!=1 & !is.na(data$SMSA)] <- 0
data$SMSA3 <- NA
data$SMSA3[data$SMSA==2 & !is.na(data$SMSA)] <- 1
data$SMSA3[data$SMSA!=2 & !is.na(data$SMSA)] <- 0
data$SMSA4 <- NA
data$SMSA4[data$SMSA==3 & !is.na(data$SMSA)] <- 1
data$SMSA4[data$SMSA!=3 & !is.na(data$SMSA)] <- 0

data$mother_seperate <- as.numeric(data$mother_seperate)-1

data$school_satis1 <- NA
data$school_satis1[(data$school_satisfaction==1 | data$school_satisfaction==2) & !is.na(data$school_satisfaction)] <- 1
data$school_satis1[data$school_satisfaction!=1 & data$school_satisfaction!=2 & !is.na(data$school_satisfaction)] <- 0
data$school_satis2 <- NA
data$school_satis2[data$school_satisfaction==3 & !is.na(data$school_satisfaction)] <- 1
data$school_satis2[data$school_satisfaction!=3 & !is.na(data$school_satisfaction)] <- 0
data$school_satis3 <- NA
data$school_satis3[data$school_satisfaction==4 & !is.na(data$school_satisfaction)] <- 1
data$school_satis3[data$school_satisfaction!=4 & !is.na(data$school_satisfaction)] <- 0

data$region1 <- NA
data$region1[data$region==1 & !is.na(data$region)] <- 1
data$region1[data$region!=1 & !is.na(data$region)] <- 0
data$region2 <- NA
data$region2[data$region==2 & !is.na(data$region)] <- 1
data$region2[data$region!=2 & !is.na(data$region)] <- 0
data$region3 <- NA
data$region3[data$region==3 & !is.na(data$region)] <- 1
data$region3[data$region!=3 & !is.na(data$region)] <- 0
data$region4 <- NA
data$region4[data$region==4 & !is.na(data$region)] <- 1
data$region4[data$region!=4 & !is.na(data$region)] <- 0

data$m_work <- as.numeric(data$m_work)-1

data$race1 <- NA
data$race1[data$race=="Other" & !is.na(data$race)] <- 1
data$race1[data$race!="Other" & !is.na(data$race)] <- 0
data$race2 <- NA
data$race2[data$race=="Black" & !is.na(data$race)] <- 1
data$race2[data$race!="Black" & !is.na(data$race)] <- 0
data$race3 <- NA
data$race3[data$race=="Hispanic" & !is.na(data$race)] <- 1
data$race3[data$race!="Hispanic" & !is.na(data$race)] <- 0

data$completion <- as.numeric(data$completion)-1

data$pincome_1 <- NA
data$pincome_1[data$parental_income_rank>=0.60 & !is.na(data$parental_income_rank)] <- 1
data$pincome_1[data$parental_income_rank<0.60 & !is.na(data$parental_income_rank)] <- 0

data$pincome_2 <- NA
data$pincome_2[data$parental_income_rank<=0.40 & !is.na(data$parental_income_rank)] <- 1
data$pincome_2[data$parental_income_rank>0.40 & !is.na(data$parental_income_rank)] <- 0

data <- data[!is.na(data$pincome_1) & (data$pincome_1==1 | data$pincome_2==1), ]
colMeans(is.na(data))
data <- na.omit(data)   # from 3295 to 2008

covariates <- c("AFQT","gender","medu","parental_presence","n_sib","urban","edu_exp","age",
                "friend_edu_exp","rotter_score","rosenberg_irt_score","foreign_lang","m_work",
                "SMSA2","SMSA3","SMSA4","mother_seperate","school_satis2","school_satis3",
                "region2","region3","region4","race2","race3","parental_income_rank")


Y="adult_income_rank"
D="completion"
G="pincome_1"
X=covariates
data=data

##### Manually run the nuisance functions of cdgd0_pa
# treatment model
DgivenGX.Model <- stats::glm(stats::as.formula(paste(D, paste(G,paste(X,collapse="+"),sep="+"), sep="~")), data=data, family=stats::binomial(link="logit"))

# treatment predictions
DgivenGX.Pred <- rep(NA, nrow(data))
DgivenGX.Pred <- stats::predict(DgivenGX.Model, newdata = data, type="response")

### outcome regression model
YgivenDGX.Model <- stats::lm(stats::as.formula(paste(Y, paste(paste(D,c(G,X),sep="*"),collapse="+"), sep="~")), data=data)

# outcome predictions
YgivenGX.Pred_D0 <- YgivenGX.Pred_D1 <- rep(NA, nrow(data))

pred_data <- data
pred_data[,D] <- 0
YgivenGX.Pred_D0 <- stats::predict(YgivenDGX.Model, newdata = pred_data)

pred_data <- data
pred_data[,D] <- 1
YgivenGX.Pred_D1 <- stats::predict(YgivenDGX.Model, newdata = pred_data)

### Diagnostics 
# covariate overlap
p_score_para <- ggplot(data.frame(value=DgivenGX.Pred), aes(x=DgivenGX.Pred)) + 
  geom_histogram(bins=50) +
  labs(title = "(d) Parametric model", x = "Propensity score", y = "Frequency")

# covariate balance after weighting
balance_para <- sapply(c(G,X), ASD)

# outcome model fit
# in order to make comparison with ML models (which are paired with cross-fitting) fair, we also use cross-fitting here to calculate the MSE
set.seed(1)
sample1 <- sample(nrow(data), floor(nrow(data)/2), replace=FALSE)
sample2 <- setdiff(1:nrow(data), sample1)

### outcome regression model
YgivenDGX.Model.sample1 <- stats::lm(stats::as.formula(paste(Y, paste(paste(D,c(G,X),sep="*"),collapse="+"), sep="~")), data=data[sample1,])
YgivenDGX.Model.sample2 <- stats::lm(stats::as.formula(paste(Y, paste(paste(D,c(G,X),sep="*"),collapse="+"), sep="~")), data=data[sample2,])

### outcome predictions
YgivenGX.Pred_D0 <- YgivenGX.Pred_D1 <- rep(NA, nrow(data))

pred_data <- data
pred_data[,D] <- 0

YgivenGX.Pred_D0[sample2] <- stats::predict(YgivenDGX.Model.sample1, newdata = pred_data[sample2,])
YgivenGX.Pred_D0[sample1] <- stats::predict(YgivenDGX.Model.sample2, newdata = pred_data[sample1,])

pred_data <- data
pred_data[,D] <- 1

YgivenGX.Pred_D1[sample2] <- stats::predict(YgivenDGX.Model.sample1, newdata = pred_data[sample2,])
YgivenGX.Pred_D1[sample1] <- stats::predict(YgivenDGX.Model.sample2, newdata = pred_data[sample1,])

Ypred <- YgivenGX.Pred_D0*(1-DgivenGX.Pred) + YgivenGX.Pred_D1*DgivenGX.Pred
mse_para <- mean((data$adult_income_rank - Ypred)^2)


##### Manually run the nuisance functions of cdgd0_ml, nnet
set.seed(1)
### estimate the nuisance functions with cross-fitting
sample1 <- sample(nrow(data), floor(nrow(data)/2), replace=FALSE)
sample2 <- setdiff(1:nrow(data), sample1)

### propensity score model
data[,D] <- as.factor(data[,D])
levels(data[,D]) <- c("D0","D1")  # necessary for caret implementation of ranger

DgivenGX.Model.sample1 <- caret::train(stats::as.formula(paste(D, paste(G,paste(X,collapse="+"),sep="+"), sep="~")), data=data[sample1,], method="nnet",
                                                                           preProc=c("center","scale"), trControl=caret::trainControl(method="cv"), linout=FALSE )
DgivenGX.Model.sample2 <- caret::train(stats::as.formula(paste(D, paste(G,paste(X,collapse="+"),sep="+"), sep="~")), data=data[sample2,], method="nnet",
                                                                           preProc=c("center","scale"), trControl=caret::trainControl(method="cv"), linout=FALSE )

data[,D] <- as.numeric(data[,D])-1

# treatment predictions
DgivenGX.Pred <- rep(NA, nrow(data))

DgivenGX.Pred[sample2] <- stats::predict(DgivenGX.Model.sample1, newdata = data[sample2,], type="prob")[,2]
DgivenGX.Pred[sample1] <- stats::predict(DgivenGX.Model.sample2, newdata = data[sample1,], type="prob")[,2]

### outcome regression model
YgivenDGX.Model.sample1 <- caret::train(stats::as.formula(paste(Y, paste(D,G,paste(X,collapse="+"),sep="+"), sep="~")), data=data[sample1,], method="nnet",
                                                                          preProc=c("center","scale"), trControl=caret::trainControl(method="cv"), linout=TRUE )
YgivenDGX.Model.sample2 <- caret::train(stats::as.formula(paste(Y, paste(D,G,paste(X,collapse="+"),sep="+"), sep="~")), data=data[sample2,], method="nnet",
                                                                          preProc=c("center","scale"), trControl=caret::trainControl(method="cv"), linout=TRUE )

### outcome predictions
YgivenGX.Pred_D0 <- YgivenGX.Pred_D1 <- rep(NA, nrow(data))

pred_data <- data
pred_data[,D] <- 0

YgivenGX.Pred_D0[sample2] <- stats::predict(YgivenDGX.Model.sample1, newdata = pred_data[sample2,])
YgivenGX.Pred_D0[sample1] <- stats::predict(YgivenDGX.Model.sample2, newdata = pred_data[sample1,])

pred_data <- data
pred_data[,D] <- 1

YgivenGX.Pred_D1[sample2] <- stats::predict(YgivenDGX.Model.sample1, newdata = pred_data[sample2,])
YgivenGX.Pred_D1[sample1] <- stats::predict(YgivenDGX.Model.sample2, newdata = pred_data[sample1,])

### Diagnostics 
# covariate overlap
p_score_nnet <- ggplot(data.frame(value=DgivenGX.Pred), aes(x=DgivenGX.Pred)) + 
  geom_histogram(bins=50) +
  labs(title = "(a) Neural networks", x = "Propensity score", y = "Frequency")

# covariate balance after weighting
balance_nnet <- sapply(c(G,X), ASD)

# outcome model fit
Ypred <- YgivenGX.Pred_D0*(1-DgivenGX.Pred) + YgivenGX.Pred_D1*DgivenGX.Pred
mse_nnet <- mean((data$adult_income_rank - Ypred)^2)


##### Manually run the nuisance functions of cdgd0_ml, gbm
set.seed(1)
data <- as.data.frame(data)

### estimate the nuisance functions with cross-fitting
sample1 <- sample(nrow(data), floor(nrow(data)/2), replace=FALSE)
sample2 <- setdiff(1:nrow(data), sample1)

### propensity score model
data[,D] <- as.factor(data[,D])
levels(data[,D]) <- c("D0","D1")  # necessary for caret implementation of ranger

DgivenGX.Model.sample1 <- caret::train(stats::as.formula(paste(D, paste(G,paste(X,collapse="+"),sep="+"), sep="~")), data=data[sample1,], method="gbm",
                                                                           trControl=caret::trainControl(method="cv"))
DgivenGX.Model.sample2 <- caret::train(stats::as.formula(paste(D, paste(G,paste(X,collapse="+"),sep="+"), sep="~")), data=data[sample2,], method="gbm",
                                                                           trControl=caret::trainControl(method="cv"))

data[,D] <- as.numeric(data[,D])-1

# treatment predictions
DgivenGX.Pred <- rep(NA, nrow(data))

DgivenGX.Pred[sample2] <- stats::predict(DgivenGX.Model.sample1, newdata = data[sample2,], type="prob")[,2]
DgivenGX.Pred[sample1] <- stats::predict(DgivenGX.Model.sample2, newdata = data[sample1,], type="prob")[,2]

### outcome regression model
YgivenDGX.Model.sample1 <- caret::train(stats::as.formula(paste(Y, paste(D,G,paste(X,collapse="+"),sep="+"), sep="~")), data=data[sample1,], method="gbm",
                                                                            trControl=caret::trainControl(method="cv"))
YgivenDGX.Model.sample2 <- caret::train(stats::as.formula(paste(Y, paste(D,G,paste(X,collapse="+"),sep="+"), sep="~")), data=data[sample2,], method="gbm",
                                                                            trControl=caret::trainControl(method="cv"))

### outcome predictions
YgivenGX.Pred_D0 <- YgivenGX.Pred_D1 <- rep(NA, nrow(data))

pred_data <- data
pred_data[,D] <- 0

YgivenGX.Pred_D0[sample2] <- stats::predict(YgivenDGX.Model.sample1, newdata = pred_data[sample2,])
YgivenGX.Pred_D0[sample1] <- stats::predict(YgivenDGX.Model.sample2, newdata = pred_data[sample1,])

pred_data <- data
pred_data[,D] <- 1

YgivenGX.Pred_D1[sample2] <- stats::predict(YgivenDGX.Model.sample1, newdata = pred_data[sample2,])
YgivenGX.Pred_D1[sample1] <- stats::predict(YgivenDGX.Model.sample2, newdata = pred_data[sample1,])

### Diagnostics 
# covariate overlap
p_score_gbm <- ggplot(data.frame(value=DgivenGX.Pred), aes(x=DgivenGX.Pred)) + 
  geom_histogram(bins=50) +
  labs(title = "(b) Generalized boosting machines", x = "Propensity score", y = "Frequency")

# covariate balance after weighting
balance_gbm <- sapply(c(G,X), ASD)

# outcome model fit
Ypred <- YgivenGX.Pred_D0*(1-DgivenGX.Pred) + YgivenGX.Pred_D1*DgivenGX.Pred
mse_gbm <- mean((data$adult_income_rank - Ypred)^2)


##### Manually run the nuisance functions of cdgd0_ml, ranger
set.seed(1)
# estimate the nuisance functions within each group, so that the final estimates are independent across groups
sample1 <- sample(nrow(data), floor(nrow(data)/2), replace=FALSE)
sample2 <- setdiff(1:nrow(data), sample1)

YgivenDGX.Model.sample1 <- caret::train(stats::as.formula(paste(Y, paste(D,G,paste(X,collapse="+"),sep="+"), sep="~")), data=data[sample1,], method="ranger",
                                                                          trControl=caret::trainControl(method="cv")) 

# outcome regression model
YgivenDGX.Model.sample1 <- caret::train(stats::as.formula(paste(Y, paste(D,G,paste(X,collapse="+"),sep="+"), sep="~")), data=data[sample1,], method="ranger",
                                                                          trControl=caret::trainControl(method="boot", number=15),
                                                                          tuneGrid=expand.grid(mtry=c(10,15,20),splitrule=c("variance"),min.node.size=c(200,250,300)))
YgivenDGX.Model.sample2 <- caret::train(stats::as.formula(paste(Y, paste(D,G,paste(X,collapse="+"),sep="+"), sep="~")), data=data[sample2,], method="ranger",
                                                                          trControl=caret::trainControl(method="boot", number=15),
                                                                          tuneGrid=expand.grid(mtry=c(10,15,20),splitrule=c("variance"),min.node.size=c(200,250,300)))

# propensity score model
data[,D] <- as.factor(data[,D])
levels(data[,D]) <- c("D0","D1")  # necessary for caret implementation of ranger

DgivenGX.Model.sample1 <- caret::train(stats::as.formula(paste(D, paste(G,paste(X,collapse="+"),sep="+"), sep="~")), data=data[sample1,], method="ranger",
                                                                         trControl=caret::trainControl(method="boot", number=15, classProbs=TRUE),
                                                                         tuneGrid=expand.grid(mtry=c(10,15,20),splitrule=c("gini"),min.node.size=c(200,250,300))) 
DgivenGX.Model.sample2 <- caret::train(stats::as.formula(paste(D, paste(G,paste(X,collapse="+"),sep="+"), sep="~")), data=data[sample2,], method="ranger",
                                                                         trControl=caret::trainControl(method="boot", number=15, classProbs=TRUE),
                                                                         tuneGrid=expand.grid(mtry=c(10,15,20),splitrule=c("gini"),min.node.size=c(200,250,300)))
data[,D] <- as.numeric(data[,D])-1

### cross-fitted predictions
YgivenGX.Pred_D0 <- YgivenGX.Pred_D1 <- DgivenGX.Pred <- rep(NA, nrow(data))

pred_data <- data
pred_data[,D] <- 0
YgivenGX.Pred_D0[sample2] <- stats::predict(YgivenDGX.Model.sample1, newdata = pred_data[sample2,])
YgivenGX.Pred_D0[sample1] <- stats::predict(YgivenDGX.Model.sample2, newdata = pred_data[sample1,])

pred_data <- data
pred_data[,D] <- 1
YgivenGX.Pred_D1[sample2] <- stats::predict(YgivenDGX.Model.sample1, newdata = pred_data[sample2,])
YgivenGX.Pred_D1[sample1] <- stats::predict(YgivenDGX.Model.sample2, newdata = pred_data[sample1,])

pred_data <- data
DgivenGX.Pred[sample2] <- stats::predict(DgivenGX.Model.sample1, newdata = pred_data[sample2,], type="prob")[,2]
DgivenGX.Pred[sample1] <- stats::predict(DgivenGX.Model.sample2, newdata = pred_data[sample1,], type="prob")[,2]

### Diagnostics 
# covariate overlap
p_score_ranger <- ggplot(data.frame(value=DgivenGX.Pred), aes(x=DgivenGX.Pred)) + 
  geom_histogram(bins=50) +
  labs(title = "(c) Random forests", x = "Propensity score", y = "Frequency")

# covariate balance after weighting
balance_ranger <- sapply(c(G,X), ASD)

# outcome model fit
Ypred <- YgivenGX.Pred_D0*(1-DgivenGX.Pred) + YgivenGX.Pred_D1*DgivenGX.Pred
mse_ranger <- mean((data$adult_income_rank - Ypred)^2)

##### pool the results together
### Covariate overlap
grid.arrange(p_score_nnet, p_score_gbm, p_score_ranger, p_score_para, ncol = 2)

### Outcome model fit
## get the MSEs for intercept-only model
set.seed(1)
sample1 <- sample(nrow(data), floor(nrow(data)/2), replace=FALSE)
sample2 <- setdiff(1:nrow(data), sample1)

Ypred[sample1] <- mean(data$adult_income_rank[sample2])
Ypred[sample2] <- mean(data$adult_income_rank[sample1])

mse_intercept <- mean((data$adult_income_rank - Ypred)^2)

c(mse_intercept, mse_para, mse_nnet, mse_gbm, mse_ranger)

### Covariate balance after weighting
## get the balance when no weighted is used
ASD_unweighted <- function(j) {
  mean1 <- mean(data[,j]*(data[,D]/mean(data[,D])))
  mean0 <- mean(data[,j]*((1-data[,D])/mean((1-data[,D]))))

  var1 <- sum(data[,D])/( (sum(data[,D]))^2 - sum((data[,D])^2) ) * sum( data[,D]*(data[,j] - mean1)^2 )
  var0 <- sum((1-data[,D]))/( sum((1-data[,D]))^2 - sum(((1-data[,D]))^2) ) * sum( (1-data[,D])*(data[,j] - mean0)^2 )

  return( abs((mean1-mean0)/sqrt((var1+var0)/2)*100) )
  }

balance_unweighted <- sapply(c(G,X), ASD_unweighted)

data_balance <- data.frame(model = rep(c("Parametric", "GBM", "Neural networks", "Random forests", "Unweighted"), each = length(balance_para)),
                           variable = rep(c(G,X), 5),
                           value = c(balance_para, balance_gbm, balance_nnet, balance_ranger, balance_unweighted))
data_balance$variable <- factor(data_balance$variable, levels = c(G,X))
data_balance$model <- factor(data_balance$model, levels = c("Unweighted", "Parametric", "Neural networks", "GBM", "Random forests")) # for ordering the legend

ggplot(data_balance, aes(x = value, y = variable, shape = model, color = model)) +
  geom_point(size = 3) +
  scale_shape_manual(values = c(16, 17, 19, 15, 18)) + 
  scale_color_manual(values = c("black", "blue", "red", "green", "purple")) + 
  labs(x = "Absolute standardized difference", y = "") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 12)) +
  geom_vline(xintercept = 20, linetype = "solid", color = "black") +
  guides(shape = guide_legend(title = ""), color = guide_legend(title = "")) 

